%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% QUESTION

Consider a prior distribution $P \sim Beta(\alpha, \beta)$ for some fixed
$\alpha$ and $\beta$ for a likelihood function $X|P \sim Bin(n, P)$. Derive
the (a) prior distribution and (b) the Bayesian point estimator.

% SOLUTION

TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% QUESTION

Consider observing $X \sim Bin(n, p)$. We know that the MLE estimator of $p$ is
given by $\hat{p}_{MLE} = X / n$. The Binomial comes from doing $n$ Bernoulli
trials and adding the number of 1s. Consider creating a new $Y$ in which we 
artificially augment the data $X$ by adding (in effect) an extra 0 and an extra
1. In other words, we create a $Y = X + 1$ with the assumption that
$Y \sim Bin(n+2, p)$. What is the MLE of $p$ using the data from the augmented
data $Y$? Where have you seen this before?

% SOLUTION

TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% QUESTION

Consider your solution to the previous two questions. If $\alpha$ and $\beta$
are non-negative integers, how could you describe the Bayesian estimator based
on adding data to $X$? 

% SOLUTION

TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% QUESTION

The standard uniform distribution is equivalent to $Beta(1,1)$. In the notes
I started by implicitly assuming that this was a fairly neutral starting
assumption for indicating that we do not have any strong prior knowledge of
$P$. Based on your results above, what would actually seem to be the best
netural position if we do not want the prior to have a strong influence on 
the posterior mean?

% SOLUTION

TODO
