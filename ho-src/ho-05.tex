\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{4}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 05: Two-Samples}

\vspace*{18pt}

\noindent
Consider extending our current setup for a statistical sample to the
case where we have two samples, where each random observation is 
mutually independent from all the others:
\begin{align*}
X_1, \ldots, X_n \iid \, \mathcal{G}_X, \quad \E[X_i] &= \mu_X, \quad \V[X_i] = \sigma_X^2 \\
Y_1, \ldots, Y_m \iid \, \mathcal{G}_Y, \quad \E[Y_i] &= \mu_Y, \quad \V[Y_i] = \sigma_Y^2
\end{align*}
We will have the normal definitions of the sample means ($\bar{X}$ and
$\bar{Y}$) and the sample variances ($S_X^2$ and $S_Y^2$). The
\textbf{pooled variance} $S_p^2$ is defined as a combination
of the sample variance of $X$ and $Y$:
\begin{align*}
S_p^2 &= \frac{(n-1) S_X^2 + (m - 1) S_Y^2}{n + m - 2}
\end{align*}
We will study its properties on today's worksheet. One of our key 
questions will how to build a point estimator and confidence interval
for the difference in the means of the two distributions:
$\mu_X - \mu_Y$.

\vspace*{20pt}

\noindent
\underline{F-distribution} \\
There is one more special distribution that we make significant usage of
in statistics that, like the T-distribution, is particularly designed to
serve as a pivot. If we have two independent random variables $C_1$ and
$C_2$ that have chi-squared distributions of $k_1$ and $k_2$ degrees of
freedome. Then, the following:
\begin{align*}
F = \frac{C_1 / k_1}{C_2 / k_2}
\end{align*}
Has an F-distribution with $k_1$ and $k_2$ degrees of freedom (yes, it 
has two different degrees of freedom). We will use this today and then
for several seemingly quite different tasks throughout the next several
weeks.

% We have fully derived the one-sample T-test. Today, we will introduce the
% \textbf{two-sample T-test}. The setup and final results are repeated here
% for easy reference. Consider observing two different random samples from
% two potentially different underlying distributions. We will write this as
% $X_1, \ldots, X_n \iid \mathcal{G}_X$ and $Y_1, \ldots, Y_m \iid \mathcal{G}_Y$. We will assume that both
% $\mathcal{G}_X$ and $\mathcal{G}_Y$ are normal and that they have a shared
% common (but unknown) variance $\sigma^2$. We want to produce an hypothesis
% test that the difference in means $\theta = \mu_X - \mu_Y$ is equal to some
% fixed value $\theta_0$ (typically zero). First, we define the pooled sample
% variance as follows: 
% \begin{align*}
% S_p^2 &= \frac{(n-1) S_X^2 + (m - 1) S_Y^2}{n + m - 2} \sim \chi^2(n + m - 2).
% \end{align*}
% Then, the following is a valid pivot:
% \begin{align*}
% T &= \frac{(\bar{X} - \bar{Y}) - (\mu_X - \mu_Y)}{S_p \cdot \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim T(n + m - 2).
% \end{align*}
% With the null value of $\theta$ plugged in, we get a valid test statistic.
% The corresponding confidence inverval for the difference means is:
% \begin{align*}
% (\bar{X} - \bar{Y}) \pm t_{1 - \alpha/2} \cdot \sqrt{\frac{S_p^2}{\frac{1}{n} + \frac{1}{m}}}
% \end{align*}
% The central limit theorem can be used to extend this result to the case where
% the distributions are not normal. In R, we will see a variant that further extends
% this to the situation where the groups have different variances.

\end{document}

