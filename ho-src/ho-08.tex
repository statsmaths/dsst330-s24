\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{4}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 08: Multiple Means}

\vspace*{18pt}

\noindent
Today we build an hypothesis test to test the null hypothesis that the
means of multiple groups (more than two) are all the same. The alternative
hypothesis is simply that at least one of the means is different from the
others. The key difficulty of this task is notational. 

Consider a set of $N$ independent random variables that are split into $K$ 
pre-defined \textbf{blocks} of sizes $n_1, \ldots, n_k$. We have the 
following assumption about the distribution of block $j$:
\begin{align*}
X_{1, j}, \ldots, X_{n_j, j} &\iid N(\mu_j, \sigma^2)
\end{align*}
Notice that the variances are all assumed to be the same and we assume from
the outset that the variables have normal distributions. The summary
statistics $\bar{X}_j$ and $S^2_j$ correspond to the sample mean and sample
variance of the $j$th block by itself. The value $\bar{X}$ is the sample
average across all observations.

As mentioned above goal is to produce a pivot for the hypothesis:
\begin{align*}
H_0:& \mu_1 = \cdots = \mu_j \\
H_A:& \exists j \exists k (\mu_j \neq \mu_k)
\end{align*}
The pivot will have an F distribution. We will derive the form on today's
worksheet. Note that unlike the previous tests we have derived, this one
does not have a direct corresponding confidence interval because they 
hypothesis is not a simple, univariate statement. The resulting test is
called a \textbf{one-way analysis of variance} or \textbf{one-way ANOVA}.

\end{document}

