\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{4}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 19: Cramér-Rao Lower Bound}

\vspace*{18pt}

\noindent
Today we derive the most theoretical result of the semester: the
\textbf{Cramér-Rao} lower bound called the on the variance of an
unbiased estimator. Since the variance of an unbiased estimator
gives the expected error of the estimator, this bound provides a
best-case scenario on how well we can estimate a parameter. We
will see on today's worksheet that many of the MLE estimators we
have so far derived achieve this lower bound.

Consider a random variable $X$ with a probability density function 
$f(\theta; x)$, which has a one univariate parameter $\theta$. We
can define a random variable $V$, called the \textbf{score function},
as the derivative of the logarithm of the density of $X$. \footnote{
  The score function is a bit unusual because we are evaluating the
  density of the random variable at the value of the random variable.
  In other words, we have $f(\theta; X)$ rather than the usual $f(\theta; x)$. 
} We see that this has a nice form by applying the chain rule:
\begin{align*}
V &= \frac{\partial}{\partial \theta} \left[ \log(f(\theta; X)) \right] \\
&= \frac{1}{f(\theta; X)} \cdot \frac{\partial}{\partial \theta} \left[ f(\theta; X) \right].
\end{align*}
The score, under some fairly general regularity conditions, will have an
expected value of zero:
\begin{align*}
\mathbb{E} V &= \int f(\theta; x) \cdot \frac{1}{f(\theta; x)} \cdot \frac{\partial}{\partial \theta} \left[ f(\theta; x) \right] dx \\
&= \int \frac{\partial}{\partial \theta} f(\theta; x) dx =
\frac{\partial}{\partial \theta} \int f(\theta; x) dx = \frac{\partial}{\partial \theta} \left[1\right] = 0.
\end{align*}
The variance of the score is called the \textbf{Fisher information}
and serves as a measurment of how much information about $\theta$ is
provided by the data $X$. This is often written as $\mathcal{I}(\theta)$.
Note that the variance is just the expected value of $V^2$.

Now, let $T = t(X)$ be a point estimator for the parameter $\theta$ with
expectation $\mathbb{E}T = \psi(\theta)$. In other words, if $T$ is
unbiased, we would have $\psi(\theta) = \theta$ for all values of 
$\theta$. If we look at the covariance of $T$ and $V$, note that this
is equal to just $\mathbb{E}[VT]$ since the expected value of $V$ is zero.\footnote{
  Recall that the covariance in general would be $\mathbb{E}[(V - \mathbb{E}V)(T - \mathbb{E}T)]$.
} This has, by construction, a nice form:
\begin{align*}
Cov(V, T) &= \int \left[ t(x) \times \frac{1}{f(\theta; x)} \times \frac{\partial}{\partial \theta} \left[ f(\theta; x) \right] \right] dx \\
&= \frac{\partial}{\partial \theta} \left[ \int t(x) f(\theta, x) dx \right] = \frac{\partial}{\partial \theta} \mathbb{E}T = \psi'(\theta).
\end{align*}
Next, we need to use the \textbf{Cauchy-Schwartz Inequality}, which for
probability spaces says that covariance of two random variables is always
less in absolute value than the square-root of the product of their
variances.\footnote{
  The more general form says that the squared inner product $| \langle u, v\rangle |^2$.
  is less than $\langle u, u\rangle \cdot \langle v, v\rangle$. Applying this to
  the integration with density $f$ yields the probabilistic version.
} Applying this to $T$ and $V$ shows that:
\begin{align*}
Var(T) \cdot Var(V) \geq | Cov(V, T) |^2  \\
Var(T) \cdot \mathcal{I}(\theta) \geq | \psi'(\theta) |^2  \\
Var(T) \geq \frac{| \psi'(\theta) |^2}{\mathcal{I}(\theta)}.
\end{align*}
Which implies, in the unbiased case, that the variance of $T$ can never be
less than the inverse of the Fisher information. So, this provides a bound
on the best that we can hope to do in terms of estimating the parameter 
$\theta$ from the data $X$.

The \textbf{efficency} of an unbiased estimator, written $e(\widehat{\theta})$, 
provides a measurement of how far away the variance of the estimator is 
away from the Cramèr-Rao bound. Namely, we have:
\begin{align*}
e(\widehat{\theta}) &= \frac{\mathcal{I}(\theta)^{-1}}{Var(\widehat{\theta})}.
\end{align*}
We say that an estimator is \textbf{efficent} if it has an effiency of $1$.
Another way to state the Cramér-Rao bound is to simply say that the efficency
is never greater than $1$.

Under some regularity conditions---in particular, that the logarithm of the
density function $f$ is twice-differentiable---the Fisher information can be
written in a somewhat simplified form:
\begin{align*}
\mathcal{I}(\theta) &= \mathbb{E} \left[ \frac{\partial^2}{\partial \theta^2} \log f(\theta; x) \right].
\end{align*}
Typically, squaring the log density requires having a number of cross terms,
whereas the second derivative removes a number of terms, simplifying the 
calculation. This is the version that we will use on the worksheet.

\vspace*{18pt}

\noindent
It is possible to extend the result above to the case where $X$ and $\theta$
are vectors. The extension for a vector $X$, which includes the important case
of a random sample of size $n$, is fairly trivial. We just replace all of the
single integrals above with $n$-dimensional integrals over $\mathbb{R}^n$. 
Generalizing to a vector value for $\theta$ is a bit more work, requiring 
some vector calculus that goes beyond the prerequisites for this course. 
The general idea, however, is very similar.


\end{document}



