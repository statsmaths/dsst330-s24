\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{4}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 12: Contingency Tables}

\vspace*{18pt}

\noindent
While there are many uses of the G-test, the most common application 
is in the study of contingency tables. Consider, for example, a multinomial
with $k=4$, just as before. However, this time we are going to arrange
the data into a two-by-two table, using a slightly different notation for
the counts to make it clear that each is associated with a specific row
and column. This yields the following, where we have added row sums $r_j$
and column sums $c_j$, since we will need them in a moment: 
\begin{center}
\begin{tabular}{*3c}
\cline{1-2}
  \multicolumn{1}{|c|}{$x_{1,1}$} &
  \multicolumn{1}{|c|}{$x_{1,2}$} &
  \multicolumn{1}{|c}{$r_1$} \\  
\cline{1-2}
  \multicolumn{1}{|c|}{$x_{2,1}$} &
  \multicolumn{1}{|c|}{$x_{2,2}$} &
  \multicolumn{1}{|c}{$r_2$} \\ 
\cline{1-2}
  \multicolumn{1}{c}{$c_1$}
  & \multicolumn{1}{c}{$c_2$}
  & \multicolumn{1}{c}{$n$}
\end{tabular}
\end{center}
We can re-define the multinomial probabilities similarly, where $p_{i,j}$
is the probability of landing in row $i$ and column $j$. A very common
type of hypothesis test is to consider the set $\Theta_0$ of all tables
in which event of being in row $i$ is independent of the event of being
in column $j$, for all combinations of $i$ and $j$.

The maximum likelihood estimator is unchanged in this case; it is still
the raw counts divided by the sample size. The numerator of the $G$ test,
however, is different. In order to be in $\Theta_0$, we need to have that
$p_{i,j}$ is equal to the probability of being in row $i$ times the probability
of being in column $j$. It should not be surprising to know then that in
order to maximize the log-likelihood under $H_0$, we use the following
probabilities and implied expected counts:
\begin{align*}
\tilde{p}_{i,j} &= \biggl(\frac{r_i}{n}\biggl) \times \biggl(\frac{c_j}{n}\biggl)
\quad \Rightarrow \quad 
e_{i,j} = \biggl(\frac{r_i \times c_j}{n}\biggl).
\end{align*}
In other words, the proportion of data that were in row $i$ times the
proportion of data that were in column $j$. From here, we use the same
formula as we have on the other page by replacing the sum of $j$ with a
double sum over both $i$ and $j$.

We can extend this same approach to the case where we have $R$ rows and
$C$ columns. What, in general, will be the degrees of freedom for $G$?
We have $CR-1$ dimensions in $\Theta$ (any set of probabilities, with 
the one restriction that the sum to $1$) and $(C-1) + (R-1)$ in $\Theta_0$
(any set of valid column probabilities and row probabilities, each having
to sum to 1). This difference factors as:
\begin{align*}
(CR-1) - (C-1) - (R-1) &= (C-1) \cdot (R-1).
\end{align*} 
So, in the common two-by-two table case, we have only a single degree of
freedom. This will grow larger for tables with more rows and/or columns.

\end{document}