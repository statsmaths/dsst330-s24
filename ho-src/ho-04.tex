\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{6}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 04: Asymptotic T Statistic}

\vspace*{18pt}

\noindent
Last time, we have derived a confidence interval using the
T-distribution for estimation of the mean of a population
distribution that we know to be normal (but do not know the
mean or variance). Today, we will apply this technique to
some actual data in R. But, in general, real data is never
perfectly normal. So, why is the approach still valid? Let's
see!

\vspace*{24pt}

\noindent
First, let's define $k_X$ to be a constant given by a scaled
version of the fourth-central moment of the population distribution:
\begin{align*}
k_X &= \V \left[ \frac{X_i - \mu_X}{\sigma_X} \right]^2.
\end{align*}
Will will assume that $k_X$ is finite. Then, in the limit of large
$n$, the following chain of relationships:\footnote{
  We write $\xrightarrow[\mathcal{P}]{}$ to indicate convergence
  in probability, and $\xrightarrow[\mathcal{D}]{}$ to indicate
  an asymptotic distribution.
}
\begin{align*}
\frac{S_X^2}{\sigma_X^2} &= \frac{1}{n-1} \sum_i \left[ \frac{X_i - \bar{X}}{\sigma_X} \right]^2
\xrightarrow[\mathcal{P}]{}  \frac{1}{n} \sum_i \left[ \frac{X_i - \mu_X}{\sigma_X} \right]^2
\xrightarrow[\mathcal{D}]{} N(1, \frac{k_X}{n}) \xrightarrow[\mathcal{P}]{} 1.
\end{align*}
We will not go into the spectic proof details here, partially
because part of the proof requires some functional analysis. At
the same time, each of the steps should seem fairly intuitive.
And the second limit is just a straightforward application of
the central limit theorem.

Now, plugging this into the formula for the $T$ statistic, we have:
\begin{align*}
T &= \frac{\frac{\mu_X - \bar{X}}{\sqrt{\sigma_X^2 / n}}}{\sqrt{\frac{(n-1)S^2_X}{\sigma_X^2 \cdot (n-1)}}}
= \frac{\frac{\mu_X - \bar{X}}{\sqrt{\sigma_X^2 / n}}}{\sqrt{\frac{S^2_X}{\sigma_X^2}}}
\xrightarrow[\mathcal{D}]{} \frac{N(0, 1)}{1} = N(0, 1).
\end{align*}
The numerator is just a straightforward application of the central
limit theorem to $\bar{X}$; the denominator comes from the chain of
relationships above. We can combine them in the ``natural'' way that
you might assume due to a result called \textbf{Slutsky's Theorem}. 

\end{document}

