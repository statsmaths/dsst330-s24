\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{2}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 02: Sample Variance}

\vspace*{18pt}

\noindent
As on the previous handout, consider a random sample of size $n$
$X_1, \ldots, X_n \iid \mathcal{G}$ from a distribution $\mathcal{G}$
with a population mean of $\mu_X$ and population variance of
$\sigma^2_X$.\footnote{
  I am intentionally repeating much of the terminology from the first handout.
  If any of the statements here are unclear, make sure to go back and lookup
  what each term means.
}
Last time we looked at the sample mean. Today, we define the
\textbf{sample variance}, a sample statistic defined and denoted by:
\begin{align*}
S^2_X &= \frac{1}{n - 1} \times \sum_{i=1}^n \left[ X_i - \bar{X} \right]^{2}.
\end{align*}
The sample mean $\bar{X}$ and sample variane $S^2_X$ are independent
random variables.\footnote{
  We will not prove this as the proof is a bit complex (it requires a n-dimenisional
  change of variabels formula) and not particularly insightful.
  }
As suggested by the name, the sample variance can be used as a good
point estimator for the population variance. On the worksheet, we will
show that it is an unbiased and consistent estimator of the population
variance.

If $\mathcal{G}$ is a normal distribution, then we can show that the
sample variance has distribution equal to a scaled chi-squared
distribution.\footnote{
  Recall that a chi-squared with $k$ degrees of freedom results from
  taking $k$ independent random variables with standard normal distributions,
  squaring them, and taking their sum.
}
Specifically, we have:
\begin{align*}
\frac{(n-1)S^2_X}{\sigma_X^2} \sim \chi^2(n-1).
\end{align*}
This will also hold in the limit of large $n$ for other distributions $\mathcal{G}$
due to the central limit theorem. 

\vspace*{24pt}

\noindent
Assume that we have two independent random variables: $Z \sim N(0, 1)$ and
$C \sim \chi^2(k)$. Then, define the following ratio between the two random
variables:
\begin{align*}
T &= \frac{Z}{\sqrt{C / k}}.
\end{align*} 
It should be clear that this random variable has a well defined distribution that
depends only on the degrees of freedom $k$ of the chi-squared distribution. The
distribution is called \textbf{Student's t-distribution} with $k$ degrees of freedom,
which is denoted by $t(k)$. As we will motivate on today's worksheet, the t-distribution
has a mean of zero, is symmetric around the origin, and will be limit to a standard
normal as the degrees of freedom limits to infinity. We will see next time how the
t-distribution allows us to combine information about the sample mean (which has a
scaled standard normal distribution) and the sample variance (which has a scaled
chi-squared distribution).

\end{document}

