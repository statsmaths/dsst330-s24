\documentclass{tufte-handout}

\usepackage{amssymb,amsmath}
% \usepackage{mathspec}
\usepackage{graphicx,grffile}
\usepackage{longtable}
\usepackage{booktabs}

\newtheorem{mydef}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\setcounter{section}{1}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\V}{\text{Var}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\cblack}{\color{Black}}
\newcommand{\cblue}{\color{MidnightBlue}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\justify

{\LARGE Handout 01: Sample Mean}

\vspace*{18pt}

\noindent
In statistics, we are typically describing a repeated measurement of a
random process. The goal is to use the observations of those measurements to
better understand the process. As a way to model this process, we 
will characterize the repeated measurements by a set of independent and
identically distributed random variables from some distribution
$\mathcal{G}$. In symbols, we have $X_1, \ldots, X_n \iid \mathcal{G}$.
We call this a \textbf{random sample} of size $n$.\footnote{
  There is much more vocabulary on today's worksheet than we typically
  have. Make sure you review the material today a few times until you
  are confident with all of the terminology. We will be using these 
  throughout the entire semester.
}
Each component $X_i$
is called an \textbf{observation}. Often, we will be concerned with the
expected value of the distribution, which is known as the
\textbf{population mean}. Similarly, the \textbf{population variance} is
the variance of the distribution $\mathcal{G}$. Typically, we will use
$\mu_X$ and $\sigma^2_X$ to stand for these population quantities. We
can drop the $X$ subscript if it is clear which distribution we are working
with.

A \textbf{statistic} or \textbf{sample statistic} is any random variable
defined as a function of a random sample. One of the most common statistics
that we will use is the \textbf{sample mean}. It is denoted and defined by:
\begin{align*}
\bar{X} &= \frac{1}{n} \times \left[X_1 + \cdots + X_n \right] = \frac{1}{n} \times \sum_{i=1}^n X_i
\end{align*}
On today's worksheet, we will show that:
\begin{align*}
\E[\bar{X}] &= \mu_X, \quad \V[\bar{X}] = \frac{\sigma^2_X}{n}.
\end{align*}
From these quantities, we can quickly see that if $\mathcal{G}$ is a normal
distribution, then $\bar{X}$ will also be a normal distribution with the 
corresponding mean and variance above. This also hold in the limit of large
$n$ for other distributions as a result of the central limit theorem.

A \textbf{point estimator} is a sample statistic used to estimate a population
parameter. Common notation of a point estimator is to put a \textit{hat} over
the parameter of interest: $\hat{\theta}$.\footnote{
  One of the key things to keep track of this semester is which quantities are
  constants (such as $\mu_X$) and which are random variables ($\hat{\mu}_X$).
  Do not let the fact that these look similar hide the fact that these are
  very different quantities.
}
The \textbf{bias} of a point estimator $\hat{\theta}$ of $\mu$ is given by 
$\E\hat{\theta} - \theta$, the difference between the expected value of the point
estimator and the quantity of interest. A point estimator is said to be
\textbf{unbiased} if the bias is equal to zero. Furthermore an estimator
$\hat{\theta}$ of $\theta$ is said to be \textbf{consistent} if for all
$\epsilon > 0$ we have:
\begin{align*}
\lim_{n\rightarrow\infty}\Prob\left[ |\hat{\theta} - \theta| > \epsilon \right] &= 0.
\end{align*}
Generally, we want to use estimators that have as small (ideally, zero) bias and that
are consistent. We will check these properties of estimators that we introduce throughout
the semester. On the worksheet, we will show that $\bar{X}$ is an unbiased and consistent
estimator for the population mean $\theta$. 

\end{document}

